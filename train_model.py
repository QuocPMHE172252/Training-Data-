import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.preprocessing import LabelEncoder
import joblib
import os
import matplotlib.pyplot as plt

# Táº¡o thÆ° má»¥c lÆ°u model náº¿u chÆ°a cÃ³
os.makedirs('utils', exist_ok=True)

# Load dá»¯ liá»‡u
print("ğŸ”„ Äang load dá»¯ liá»‡u...")
data = np.load('sign_language_data.npz')
X_train, X_test = data['X_train'], data['X_test']
y_train, y_test = data['y_train'], data['y_test']

# MÃ£ hÃ³a nhÃ£n
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)
joblib.dump(label_encoder, 'utils/label_encoder.pkl')

# One-hot encoding
num_classes = len(np.unique(y_train))
y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes)
y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes)

print(f"ğŸ“Š KÃ­ch thÆ°á»›c dá»¯ liá»‡u:")
print(f"Training: {X_train.shape}")
print(f"Testing: {X_test.shape}")

# XÃ¢y dá»±ng mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£
model = models.Sequential([
    # Block 1
    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=X_train.shape[1:]),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    
    # Block 2
    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    
    # Block 3
    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    
    # Dense layers
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])

# TÃ³m táº¯t mÃ´ hÃ¬nh
model.summary()

# Compile
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Callbacks
callbacks = [
    EarlyStopping(
        patience=5,
        restore_best_weights=True,
        monitor='val_accuracy'
    ),
    ModelCheckpoint(
        'utils/best_model.keras',
        save_best_only=True,
        monitor='val_accuracy'
    )
]

print("\nğŸš€ Báº¯t Ä‘áº§u training...")

# Training
history = model.fit(
    X_train, y_train_cat,
    validation_data=(X_test, y_test_cat),
    epochs=20,
    batch_size=32,
    callbacks=callbacks
)

# ÄÃ¡nh giÃ¡ trÃªn táº­p test
test_loss, test_acc = model.evaluate(X_test, y_test_cat)
print(f"\nâœ… Accuracy trÃªn táº­p kiá»ƒm tra: {test_acc:.4f}")

# LÆ°u mÃ´ hÃ¬nh
model.save('utils/sign_language_model.keras')
print("âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh táº¡i utils/sign_language_model.keras")

# Váº½ biá»ƒu Ä‘á»“ huáº¥n luyá»‡n
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title("Biá»ƒu Ä‘á»“ Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title("Biá»ƒu Ä‘á»“ Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig("utils/training_history.png")
print("ğŸ“Š ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ táº¡i utils/training_history.png")
